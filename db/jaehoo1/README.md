# 성능을 좌우하는 DB 설계와 쿼리
## 조회 트래픽을 고려한 인덱스 설계
일반적인 시스템에서는 조회 기능의 실행 비율이 높다. 예를 들어 다수의 사용자는 게시판에서 글을 읽는다. 그중 일부 사용자만 새로운 게시글을 등록하거나 수정하고 삭제한다.

같은 게시판이어도 트래픽 규모가 다를 수 있다. 인기 있는 커뮤니티 사이트에서 사용하는 게시판과 회사 내부에서 사용하는 공지 사항 게시판은 사용자 수와 새로 등록되는 게시글 수에서 차이가 난다.

DB 테이블을 설계할 때는 조회 기능과 트래픽 규모를 고려해야 한다. 이를 고려하지 않으면 성능에 심각한 문제가 발생할 수 있다. 예를 들어 카테고리별로 나눠서 게시글 목록을 보여주는 게시판을 만들어야 한다고 하자. 게시판 테이블은 아래 표 처럼 카테고리를 저장하기 위해 category 칼럼을 가질 것이다.
<table>
    <tr>
        <th style="text-align: center" colspan="2">article</th>
    </tr>
    <tr>
        <td>id</td><td><code>integer(10)</code></td>
    </tr>
    <tr>
        <td>category</td><td><code>integer(10)</code></td>
    </tr>
    <tr>
        <td>writerId</td><td><code>integer(10)</code></td>
    </tr>
    <tr>
        <td>title</td><td><code>varchar(255)</code></td>
    </tr>
    <tr>
        <td>content</td><td><code>clob</code></td>
    </tr>
    <tr>
        <td>regdt</td><td><code>timestamp</code></td>
    </tr>
</table>

카테고리별로 분류하는 기능을 구현한 쿼리는 다음과 같이 조건절에서 category 칼럼을 비교해야 한다.
```sql
select id, category, writerId, title, content
from article
where category = #{category}     -- 카테고리 비교
order by id desc
limit 20, 10;
```

category 칼럼에 인덱스가 없다면? 테이블에 풀 스캔 발생  
→ article 테이블에 row 수가 많다면 DB가 제기능을 못하게 됨

> ### 풀 스캔(full scan)
>
> 풀 스캔은 테이블의 모든 데이터를 순차적으로 읽는 것을 말한다. 보통 **쿼리의 `where` 절에 있는 조건에 대응하는 인덱스가 없을 때 풀 스캔이 발생**한다. 인덱스를 사용하는 것보다 전체 데이터를 탐색하는 것이 더 빠를 때도 풀 스캔이 발생한다. 데이터 개수가 적을 때에는 풀 스캔을 해도 성능 문제가 겉으로 드러나지 않지만 데이터 개수가 늘어나면 어느 순간 응답 시간이 기하급수적으로 증가하게 된다. 따라서 DB를 설계할 때에는 풀 스캔 발생 가능성을 항상 염두에 두어야 한다.

풀 스캔이 발생하지 않도록 하려면 조회 패턴을 기준으로 인덱스를 설계해야 한다. 게시판의 경우, 카테고리별로 게시글 목록을 조회하는 패턴이 존재하므로 category 칼럼에 인덱스를 추가해서 조회 성능을 개선할 수 있다.

내가 작성한 글 목록 보기 기능도 비슷하다. 내가 작성한 글 목록을 조회하려면 아래 형태의 쿼리를 실행한다. 풀 스캔이 발생하지 않도록 하려면 writerId 칼럼을 포함한 인덱스를 생성해야 한다.
```sql
select id, category, writerId, title, content
from article
where writerId = #{userId}          -- 작성자 == 사용자(나)
order by id desc
limit 10, 10;
```

> ### 전문 검색 인덱스
>
> 문자열을 이용한 검색 기능은 흔히 볼 수 있는 기능이다. 제목에 특정 단어가 포함된 게시글을 검색하는 기능이 이에 해당한다. 다음 쿼리처럼 `like`를 이용해서 조건을 지정하면 된다.
> ```sql
> select id, category, writerId, title, content
> from article
> where title like '%검색어%'
> order by id desc
> limit 10;
> ```
>
> 그런데 중간에 포함된 단어를 검색하기 위한 **`like` 조건은 풀 스캔을 유발한다.**
>
> 엘라스틱서치 같은 검색 엔진을 사용하면 DB를 사용하지 않고 검색 기능을 구현할 수 있다. 하지만 별도의 검색 엔진을 구성하기 힘든 상황이라면 DB가 제공하는 전문(full-text) 검색 기능 사용을 고려해보자. Oracle Text나 MySQL의 FULLTEXT 인덱스를 사용하면 풀 스캔 없이 문자열 검색 쿼리를 실행할 수 있다.

### 단일 인덱스와 복합 인덱스
사용자의 모든 활동 내역을 보관하기 위해 아래와 같은 activityLog 테이블을 만들었다고 하자.
<table>
    <tr>
        <th style="text-align: center" colspan="2">activityLog</th>
    </tr>
    <tr>
        <td>id</td><td><code>integer(10)</code></td>
    </tr>
    <tr>
        <td>userId</td><td><code>integer(10)</code></td>
    </tr>
    <tr>
        <td>activityType</td><td><code>varchar(20)</code></td>
    </tr>
    <tr>
        <td>activityDate</td><td><code>date</code></td>
    </tr>
    <tr>
        <td>activityDatetime</td><td><code>timestamp</code></td>
    </tr>
    <tr>
        <td>memo</td><td><code>varchar(200)</code></td>
    </tr>
</table>

특정 사용자의 일자별 활동 내역을 조회하는 목적으로 activityLog 테이블을 사용한다고 가정한다. 다음 쿼리를 사용해서 활동 내역을 조회할 것이다.
```sql
select *
from activityLog
where userId = #{userId}
    and activityDate = #{activityDate}
order by activityDatetime desc;
```

이 쿼리는 특정 userId와 activityDate인 데이터를 검색한다. 성능 문제가 없으려면 userId를 포함한 인덱스가 필요하다. 여기서 고민할 점은 activityDate를 포함하느냐 하지 않느냐에 대한 것이다.
- 단일 인덱스 : userId만 인덱스로 사용
- 복합 인덱스 : (userId, activityDate)를 인덱스로 사용

회원들의 활동성이 좋다면 (테이블에 row를 많이 적재하면) (userId, activityDate) 칼럼을 조합한 복합 인덱스 사용을 고려해야 한다. 사용자당 수만 건이 넘는 데이터가 쌓일 수 있기 때문에 userId 칼럼만 인덱스로 생성하면 조회 속도가 느려질 수 있다. 이때는 userId 칼럼과 activityDate 칼럼을 복합 인덱스로 생성해야 조회 성능 문제가 발생하지 않는다.

activityLog 테이블은 통계를 추출하기 위한 목적으로도 사용할 수 있다. 예들 들어, 다음 쿼리를 이용하면 특정 일자의 활동 타입별 개수를 구할 수 있다.
```sql
select activityDate, activityType, count(activityType)
from activityLog
where activityDate = #{activityDate}
group by activityType;
```

이 쿼리를 실행할 때 전체 데이터를 풀 스캔하지 않으려면 activityDate 칼럼을 인덱스로 사용해야 한다. 쿼리 실행 빈도와 실행 시간을 검토해서 포함 여부를 결정해야 한다. (실행 빈도가 적다면 굳이 인덱스에 포함시킬 이유가 있는가?)

### 선택도를 고려한 인덱스 칼럼 선택
인덱스를 생성할 때는 선택도(selectivity)가 높은 칼럼을 골라야 한다. **선택도**는 **인덱스에서 특정 칼럼의 고유한 값 비율**을 나타낸다. 선택도가 높을수록 인덱스를 이용한 조회 효율이 높아진다.

아래에 표시한 회원 테이블을 보자.
```sql
create table member (
    memberId int not null primary key,
    gender char(1),
    ...
);
```

gender 칼럼이 M, F, N의 3개 값 중 하나를 갖고, gender 칼럼을 인덱스로 사용한다고 하자. 전체 회원 데이터 중에서 M이 50만 개, F가 50만 개, N이 천 개일 때 다음 쿼리를 실행하면 여전히 50만 개의 데이터를 확인해야 한다. 선택도가 낮아 인덱스 효율이 떨어진다.
```sql
select *
from member
where gender = 'F'
    and birthyear = #{birthyear};
```

인덱스로 사용할 칼럼을 고를 때 선택도가 항상 높아야 하는 것은 아니다. 선택도가 낮아도 인덱스 칼럼으로 적합한 상황도 있다. 작업 큐를 구현한 테이블이 이에 해당한다. 작업 큐로 사용할 테이블을 다음과 같이 정의했다고 하자.
```sql
create table jobqueue (
    jobid varchar(16) not null primary key,
    status char(1) not null,
    ...
);
```

jobqueue 테이블의 status 칼럼은 W(대기), P(처리 중), C(완료) 세 값을 갖는다. 작업 큐의 특성상 대부분 데이터의 status 칼럼값은 C이고, 적은 수의 데이터만 W와 P를 값으로 갖는다. 고유한 값이 3개 뿐이므로 선택도가 낮은 칼럼이다.

하지만 실제 사용하는 쿼리를 보면 status 칼럼은 인덱스로 사용하기에 좋은 칼럼이다. 작업 큐를 처리하는 코드는 status 칼럼값이 W인 데이터를 조회하기 때문이다.
```sql
select *
from jobqueue
where status = 'W'
order by jobid asc;
```

작업 실행기는 이 쿼리를 반복해서 실행하므로 이 쿼리가 오래 걸리면 모든 작업 실행이 지연되는 문제가 발생한다. status 칼럼에 인덱스가 걸려 있지 않으면 이 쿼리는 풀 스캔을 발생시키므로, 선택도에 상관없이 status 칼럼을 인덱스로 추가해야 한다.

### 커버링 인덱스 활용하기
**커버링 인덱스**는 **특정 쿼리를 실행하는 데 필요한 칼럼을 모두 포함하는 인덱스**를 말한다. 커버링 인덱스를 사용하면 쿼리 실행 효율을 높일 수 있다. 예를 들어 다음 쿼리를 실행한다고 해보자.
```sql
select *
from activityLog
where activityDate = #{activityDate}
    and activityType = #{activityType};
```

activityDate 칼럼과 activityType 칼럼을 사용하는 인덱스가 있다면, 인덱스를 사용해서 읽어올 데이터를 빠르게 선택할 수 있다. 데이터를 선택한 뒤에는 칼럼값을 조회하기 위해 각 데이터를 읽어 온다. 인덱스를 사용해서 조회할 데이터를 선택하는 과정은 빠르지만 실제 데이터 자체는 읽어와야 하는 것이다.
```sql
select activityDate, activityType
from activityLog
where activityDate = #{activityDate}
    and activityType = #{activityType};
```

이 쿼리는 실제 데이터에 접근하지 않는다. 왜냐하면 쿼리를 실행하는 데 필요한 activityDate 칼럼과 activityType 칼럼값이 모두 인덱스에 포함되어 있기 때문이다. 실제 데이터를 읽어오는 과정이 생략되므로 쿼리 실행 시간이 빨라진다.

## 몇 가지 조회 성능 개선 방법
### 미리 집계하기
### 페이지 기준 목록 조회 대신 ID 기준 목록 조회 방식 사용하기
### 조회 범위를 시간 기준으로 제한하기
### 전체 개수 세지 않기
목록을 표시하는 기능은 전체 개수를 함께 표시하는 경우가 많다. 조건에 해당하는 데이터 개수를 구하기 위해서는 `count` 함수를 사용해야 한다. 예를 들어, 이름에 특정 단어가 포함된 회원 목록을 조회하려면 다음 두 쿼리를 실행해야 한다.
```sql
select id, ...
from member
where name like #{name}
order by id desc
limit 20;

select count(*)
from member
where name like #{name};
```

데이터가 적을 때는 `count` 쿼리를 실행해도 큰 문제가 없다. 문제는 데이터가 급격히 증가하기 시작할 때 발생한다. 데이터가 많아질수록 `count` 실행 시간도 증가하는데, 그 이유는 조건에 해당하는 모든 데이터를 탐색해야 하기 때문이다. [커버링 인덱스](#커버링-인덱스-활용하기)를 사용하더라도 전체 인덱스를 스캔해야 하며, 커버링 인덱스가 아닌 경우에는 실제 데이터를 전부 읽어야 한다.

### 오래된 데이터 삭제 및 분리 보관하기
> ### 단편화와 최적화
> `DELETE` 쿼리를 이용해 테이블에서 데이터를 삭제하면, 실제 사용하는 디스크 용량도 줄어들까?
>
> 일반적인 DB에서는 `DELETE` 쿼리를 실행하더라도 DB가 사용하는 디스크 용량은 줄어들지 않는다. DB는 해당 데이터가 삭제되었다는 표시만 남기고, 삭제된 공간은 향후 재사용한다.
>
> 하지만 데이터가 반복적으로 추가되고, 변경되고, 삭제되는 과정에서 데이터가 흩어져 저장되고 빈 공간이 생기는 단편화(fragmentation) 현상이 발생할 수 있다.
>
> 단편화가 심해지면 디스크 I/O가 증가하면서 쿼리 성능이 저하될 수 있다. 또한 테이블에 실제로 보관된 데이터 크기보다 더 많은 디스크 공간을 사용하게 되어, 디스크 낭비도 발생한다.
>
> 단편화로 인한 성능 저하를 해결하는 방법 중 하나는 최적화 작업이다. 최적화는 데이터를 재배치해 단편화를 줄이고, 물리적인 디스크 사용량까지 줄여주는 효과가 있다.

### DB 장비 확장하기
수직 / 수평 확장

DB를 수평으로 확장하면 DB가 처리할 수 있는 트래픽을 늘릴 수 있다. 조회 트래픽 비중이 높은 서비스의 경우, 주 DB-복제 DB(Primary-Replica) 구조를 사용해 처리량을 효과적으로 증가시킬 수 있다.

![](/assets/images/jaehoo1/db/db-replica.png)

서버는 데이터 변경 쿼리(CUD, `insert` / `update` / `delete`)는 주 DB를 통해, 조회 쿼리(R, `select`)는 복제 DB를 통해 실행한다. 조회 기능에 대한 트래픽이 증가하면, 복제 DB를 추가해 조회 처리량을 확장할 수 있다.

> 보통 DB 서버는 API 서버에 비해 몇 배 이상 사양이 좋은 장비를 사용한다. 사양이 좋은 만큼 가격도 많이 비싸다. 일시적으로 급증하는 조회 트래픽에 대비해 DB 장비를 수평 확장하면 고정 비용도 함께 증가한다. 한 번 증가한 고정 비용은 지속적으로 부담이 되기 때문에 DB 서버를 확장할 때는 비용 대비 얻는 이점이 확실해야 한다.

### 별도 캐시 서버 구성하기
대규모 트래픽이 발생하는 많은 서비스는 캐시 서버를 기본적으로 사용하고 있다. DB만으로는 요청을 감당하는 데 한계가 있기 때문이다. 꼭 대규모 트래픽이 아니더라도, 캐시 서버를 잘 활용하면 DB 확장 대비 적은 비용으로 더 많은 트래픽을 처리할 수 있다. 서버 개발자나 인프라 엔지니어 입장에서도 DB를 확장하는 것보다 Redis와 같은 캐시 서버를 구성하는 것이 상대적으로 부담이 적다.

물론 캐시를 도입하면 코드를 수정해야 한다. 하지만 코드 수정에 드는 비용 대비 캐시로 증가시킬 수 있는 처리량이 크다면, 코드를 수정하는 것이 더 합리적인 선택이다.

## 알아두면 좋을 몇 가지 주의 사항
### 쿼리 타임아웃
[응답 시간](/etc/jaehoo1/README.md#느려진-서비스-어디부터-봐야-할까---응답-시간)은 처리량에 큰 영향을 준다. 동시 사용자가 증가할 때 **응답 시간이 길어지**면 그에 반비례해 처리량은 감소한다. 하지만 단순히 처리량만 떨어지는 데서 끝나지 않는다.

예를 들어, 동시 접속이 증가하면서 특정 쿼리의 실행 시간이 15초 이상으로 늘어났다고 해보자. **사용자는** 몇 초만 지나도 **서비스가 느리다고 느끼고 다시 몇 초 후에 재시도를 하게 된다.**

응답 지연으로 인한 재시도는 서버 부하를 더욱 가중시킨다. 앞선 요청을 아직 처리 중인 상황에서 새로운 요청이 유입되기 때문이다. 이런 식으로 재시도가 반복되면 동시에 처리해야 하는 요청 수가 기하급수적으로 늘어나고 서버 부하는 폭증하게 된다.

이런 상황을 방지하는 방법 중 하나는 쿼리 실행 시간을 제한(타임아웃 설정)하는 것이다. 예를 들어 쿼리 실행 시간을 5초로 제한했다고 하자. 트래픽이 증가해 쿼리 실행 시간이 5초를 넘기면 제한 시간 초과로 에러가 발생한다. 사용자는 에러 화면을 보게 되지만 서버 입장에서는 해당 요청을 정상적으로 종료(처리)한 셈이다. 사용자가 재시도를 하더라도 이전 요청이 여전히 처리 중인 상태가 아니므로 동시 요청 수의 폭증을 막을 수 있다.

쿼리 타임아웃은 서비스와 기능의 특성에 따라 다르게 설정해야 한다. 예를 들어 블로그 글을 조회하는 기능은 타임아웃을 몇 초 이내로 짧게 설정해도 되지만, 상품 결제 기능은 보다 긴 타임아웃이 필요하다. 결제 처리 중 타임아웃으로 에러가 발생하면 후속 처리와 데이터 정합성이 복잡해질 수 있기 때문이다.

### 상태 변경 기능은 복제 DB에서 조회하지 않기
[주 DB-복제 DB 구조](#db-장비-확장하기)를 사용할 때 변경은 주 DB를 사용하고 조회는 복제 DB를 사용한다. 그런데 이를 잘못 이해해 모든 `SELECT` 쿼리를 무조건 복제 DB에서 실행하는 경우가 있다. 이는 2가지 측면에서 문제를 일으킬 수 있다.

첫째, 주 DB와 복제 DB는 순간적으로 데이터가 일치하지 않을 수 있다. 주 DB에서 변경된 데이터는 다음 두 단계를 거쳐 복제 DB에 반영된다.
- 네트워크를 통해 복제 DB에 전달
- 복제 DB는 자체 데이터에 변경 내용을 반영

이 과정에는 시간이 걸린다. 주 DB에서 복제 DB로의 데이터 복제에는 지연이 발생한다. 이 지연 시간만큼 주 DB와 복제 DB는 일시적으로 서로 다른 값을 갖게 된다.

![](/assets/images/jaehoo1/db/crud-in-db-replica.png)

1.1에서 주 DB의 데이터를 변경하고, 1.2에서 변경된 데이터를 조회하는 상황이다. 복제 과정에서 지연이 발생할 수 있기 때문에, 주 DB에서 변경한 데이터가 복제 DB에 반영되기 전에 복제 DB에서 `SELECT` 쿼리가 실행될 수 있다. 이 경우 잘못된 데이터를 조회하게 되어 사용자의 요청을 제대로 처리할 수 없게 된다.

둘째, 트랜잭션 문제가 발생할 수 있다. 주 DB와 복제 DB간 데이터 복제는 트랜잭션 커밋 시점에 이뤄진다. 주 DB의 트랜잭션 범위 내에서 데이터를 변경하고, 복제 DB에서 변경 대상이 될 수 있는 데이터를 조회하면 데이터 불일치로 인해 문제가 생긴다.

`INSERT`, `UPDATE`, `DELETE` 쿼리를 실행하는 기능에서 변경 대상 데이터를 조회해야 한다면, 복제 DB가 아닌 `INSERT`, `UPDATE`, `DELETE` 쿼리를 실행하는 주 DB에서 `SELECT` 쿼리를 실행하자. 그래야 데이터 불일치로 인해 발생할 수 있는 오류를 방지할 수 있다.

### 배치 쿼리 실행 시간 증가
배치 프로그램은 데이터를 일괄로 조회하거나 집계하거나 생성하는 작업을 수행한다. 이때 사용하는 쿼리는 `group by`로 테이블을 묶고 `count`, `sum` 등을 이용해 집계를 구한다. 조건을 충족하는 데이터의 칼럼값을 일괄로 변경하는 쿼리도 자주 사용된다.

데이터가 많아질수록 일괄 처리용 쿼리의 실행 시간도 함께 증가한다. 데이터 증가에 따라 몇 십 분 안에 끝나던 쿼리가 어느 순간 몇 시간이 지나도 끝나지 않는 상황이 발생할 수 있다.

가장 빠른 해결책은 DB 장비의 사양을 높이는 것이다. 하지만 이 방법은 항상 가능한 것은 아니므로 다른 방법을 함께 고려해야 한다. 다음은 적용해 볼 수 있는 2가지 대안이다.
- [커버링 인덱스 활용](#커버링-인덱스-활용하기)
- 데이터를 일정 크기로 나눠 처리

집계 쿼리는 특성상 많은 데이터를 스캔한다. 이때 집계 대상 칼럼이 인덱스에 포함되어 있다면, 데이터를 직접 읽지 않고 인덱스만 스캔해 집계를 수행할 수 있다. 커버링 인덱스를 활용하면 처리 속도는 빨라지고 DB가 사용하는 메모리도 줄어든다.

데이터를 일정 크기로 나눠 처리하는 것도 해결책이다. 예를 들어 접속 로그를 이용해 한 달간의 다양한 통계 데이터를 추출해야 한다고 하자. 이를 위해 다음과 같은 쿼리를 사용할 수 있다.
```sql
select ...
from accesslog al
where al.accessDatetime >= '2025-10-01 00:00:00'
    and al.accessDatetime < '2025-11-01 00:00:00'
group by ...
;
```

accessDatetime 칼럼에 인덱스를 추가하더라도 데이터 개수가 많아지면 쿼리 실행에 시간이 오래 걸릴 수 있다. 특정 임계점을 넘어서면 체감상 아무리 기다려도 쿼리가 끝나지 않는 상황이 발생할 수 있다.

이럴 때는 쿼리를 시간 구간별로 나눠 실행하면 적어도 결과를 끊김 없이 구할 수 있다.
```sql
select ...
from accesslog al
where al.accessDatetime >= '2025-10-01 00:00:00'
    and al.accessDatetime < '2025-10-02 00:00:00'
group by ...
;
```

위 쿼리처럼 집계를 하루 단위로 나눠 1일부터 말일까지 실행하고 각 결과를 다시 합치면 한달의 전체 결과를 만들 수 있다. 하루 단위도 길다고 판단되면 1시간이나 10분처럼 더 짧은 간격으로 나눠 실행하면 된다. 이렇게 하면 쿼리 실행 시간을 일정 수준으로 유지할 수 있다.

### 타입이 다른 칼럼 조인 주의
<table>
    <tr>
        <th style="text-align: center" colspan="2">user</th>
    </tr>
    <tr>
        <td>userId</td><td><code>integer(10)</code></td>
    </tr>
    <tr>
        <td>name</td><td><code>varchar(20)</code></td>
    </tr>
</table>
<table>
    <tr>
        <th style="text-align: center" colspan="2">push</th>
    </tr>
    <tr>
        <td>id</td><td><code>integer(10)</code></td>
    </tr>
    <tr>
        <td>receiverType</td><td><code>varchar(2)</code></td>
    </tr>
    <tr>
        <td>receiverId</td><td><code>varchar(200)</code></td>
    </tr>
</table>

receiverId 칼럼에 인덱스를 추가했다. 이 상태에서 다음 쿼리를 실행하면 어떻게 될까?
```sql
select u.userId, u.name, p.*
from user u,
    push p
where u.userId = #{userId}
    and u.userId = p.receiverId
    and p.receiverType = #{receiverType}
order by p.id desc
limit 100;
```

이 쿼리는 먼저 user 테이블에서 주요 키인 userId 칼럼을 기준으로 비교하고, push 테이블의 인덱스가 설정된 receiverId 칼럼과 조인하기 때문에 인덱스를 사용할 것처럼 보인다. 하지만 실제로는 인덱스를 활용하지 못할 수도 있다. 그 이유는 두 칼럼의 타입이 서로 다르기 때문이다.
- user 테이블의 userId 칼럼은 `integer` 타입
- push 테이블의 receiverId 칼럼은 `varchar` 타입

두 칼럼의 값을 비교하는 과정에서 DB는 타입 변환을 수행한다. receiverId 칼럼은 `varchar` 타입이므로 userId(`integer`)와 비교하려면 DB는 receiverId 값을 `integer` 타입으로 변환해야 한다. 이 **변환은 각 행마다 발생**하며, 결과적으로 receiverId 인덱스를 온전히 활용하지 못하게 된다.

전체 데이터를 스캔하지 않고 인덱스만 스캔하는 경우에도 push 테이블의 데이터가 많다면 이 변환 작업으로 인해 쿼리 실행 시간이 길어질 수밖에 없다.

비교하는 칼럼의 타입이 달라서 인덱스를 활용하지 못하는 문제를 해결하려면 두 칼럼의 타입을 맞춰서 비교해야 한다. 다음은 MySQL에서 타입을 변환해 두 칼럼의 타입을 일치시킨 후 비교하는 예시다.
```sql
select u.userId, u.name, p.*
from user u,
    push p
where u.userId = #{userId}
    and cast(u.userId as char character set utf8mb4) collate 'utf8mb4_unicode_ci' = p.receiverId
    and p.receiverType = #{receiverType}
order by p.id desc
limit 100;
```

이처럼 **비교 대상 칼럼의 타입을 맞추면 쿼리 실행 중 발생하는 불필요한 타입 변환을 줄일 수 있고 실행 시간이 길어지는 문제도 방지할 수 있다.**

> 문자열 타입을 비교할 때는 칼럼의 캐릭터셋이 같은지 확인해야 한다. 캐릭터셋이 다르면 그 자체로도 변환이 발생할 수 있기 때문이다.

### 테이블 변경은 신중하게
데이터가 많은 테이블에 새로운 칼럼을 추가하거나 기존 열거 타입 칼럼을 변경할 때는 매우 주의해야 한다. 칼럼을 변경했다가 서비스가 장시간 중단되는 상황이 발생할 수 있다.

테이블 변경 시 주의해야 하는 이유는 DB의 테이블 변경 방식 때문이다. 예를 들어 MySQL은 테이블을 변경할 때 새 테이블을 생성하고 원본 테이블의 데이터를 복사한 뒤, 복사가 완료되면 새 테이블로 대체한다. 이 복사 과정에서는 `UPDATE`, `INSERT`, `DELETE` 같은 DML 작업을 허용하지 않기 때문에 복사 시간만큼 서비스가 멈춘다.

DML을 허용하면서 테이블을 변경하는 기능도 있지만 항상 가능한 것은 아니다. 그래서 데이터가 많은 테이블은 점검 시간을 잡고 변경하는 경우가 많다. 서비스를 잠시 중단한 뒤 변경 작업을 수행하는 것이 가장 안정적이기 때문이다.

회사에 DBA가 있다면 점검을 잡아 변경할지 아니면 서비스 제공 중에도 가능한지 판단해줄 수 있다. 하지만 DBA가 없다면 스스로 판단해야 하므로 더욱 신중해야 한다. 함부로 변경했다가 재앙이 일어날 수 있다.

## 실패와 트랜잭션 고려하기
모든 코드가 항상 정상적으로 동작하는 것은 아니기 때문에 비정상 상황에서의 트랜잭션 처리를 반드시 고민해야 한다. 트랜잭션을 고려하지 않고 코드를 작성하면 데이터 일관성에 문제가 생길 수 있다.

DB 관련 코드를 작성할 때는 트랜잭션의 시작과 종료 경계를 명확히 설정했는지 반드시 확인해야 한다.

![](/assets/images/jaehoo1/db/multi-table-crud-without-transaction.png)

위 그림은 트랜잭션 없이 DB를 변경하는 예시를 보여준다. 이 전체 과정을 트랜잭션 없이 실행한다.  
과정4에서 userStatus 테이블 변경이 실패하면 contract 테이블에는 이미 데이터가 추가된 상태지만, userStatus 테이블에는 상태가 반영되지 않는다. 사용자 입장에서는 계약 상태가 변경되지 않았으므로 다시 계약을 시도하게 된다. 그러나 이미 contract 테이블에는 데이터가 존재하므로 계약을 더 이상 진행할 수 없게 된다. 이 문제는 트랜잭션을 제대로 설정했으면 발생하지 않는다.

보통은 일부 기능에서 오류가 나면 전체 트랜잭션을 롤백한다. 하지만 경우에 따라 일부 기능에서 오류가 나도 트랜잭션을 커밋해야 할 상황도 있다.

회원 가입 기능을 예로 들어보자. 이 기능은 다음처럼 member 테이블에 회원 정보를 추가한 뒤, 가입 환영 메일을 전송한다. `mailClient.sendMail()` 메서드는 메일 발송 중 문제가 생기면 `RuntimeException`을 발생시키도록 구현되어 있다고 가정하자.
```java
@Transactional  // 트랜잭션 범위
public void join(JoinRequest join) {
    ...
    memberDao.insert(member);   // DB에 데이터 추가
    mailClient.sendMail(...);   // 메일 발송
}
```

메일 서버에 일시적인 문제가 있어 `sendMail()`에서 `RuntimeException`이 발생하면 어떻게 될까. 스프링의 `@Transactional`은 런타임 예외가 발생하면 전체 트랜잭션을 롤백한다. 따라서 위 코드는 DB에 회원 데이터를 정상적으로 추가했더라도 메일 발송 중 예외가 발생하면 회원 가입 전체가 실패하게 된다.

만약 메일 발송에 실패하더라도 회원 가입은 정상 처리되길 원한다면, 메일 발송 오류는 다음과 같이 별도로 처리해서 무시해야 한다.
```java
@Transactional  // 트랜잭션 범위
public void join(JoinRequest join) {
    ...
    memberDao.insert(member);   // DB에 데이터 추가
    try {
        mailClient.sendMail(...);   // 메일 발송
    } catch (Exception ex) {
        // 메일 발송 오류 무시
        // 로그로 기록해 모니터링
    }
}
```

외부 API 연동과 DB 작업이 섞이면 트랜잭션 처리가 복잡해진다. 외부 API 호출은 성공했지만 DB 작업이 실패하는 상황이 대표적이다. 외부 연동은 상황에 따라 대응 방법이 달라진다.